# ModelosLLM
Guía en Español de los TOP LLM  (grandes modelos lenguaje; Large Language Models)  para inteligencia artificial disponibles


# Modelos de Lenguaje de Aprendizaje Automático Abiertos (Open LLMs)

Estos modelos de lenguaje de aprendizaje automático (LLMs, por sus siglas en inglés) están todos licenciados para uso comercial (por ejemplo, Apache 2.0, MIT, OpenRAIL-M). ¡Las contribuciones son bienvenidas!

| Modelo de Lenguaje | Fecha de Lanzamiento | Checkpoints | Artículo/Blog | Parámetros (B) | Longitud del Contexto | Licencia | Pruébalo                                                                                                              |
| --- | --- | --- | --- | --- | --- | --- |---------------------------------------------------------------------------------------------------------------------|
| T5           | 2019/10 |[T5 & Flan-T5](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints), [Flan-T5-xxl (HF)](https://huggingface.co/google/flan-t5-xxl)      | [Explorando los Límites del Aprendizaje por Transferencia con un Transformador Texto-a-Texto Unificado](https://github.com/google-research/text-to-text-transfer-transformer#released-model-checkpoints) | 0.06 - 11       | [512](https://discuss.huggingface.co/t/does-t5-truncate-input-longer-than-512-internally/3602) | Apache 2.0         | [T5-Large](https://github.com/slai-labs/get-beam/tree/main/examples/t5)                                               |
| UL2          | 2022/10 | [UL2 & Flan-UL2](https://github.com/google-research/google-research/tree/master/ul2#checkpoints), [Flan-UL2 (HF)](https://huggingface.co/google/flan-ul2)          | [UL2 20B: Un Aprendiz del Lenguaje Unificado de Código Abierto](https://ai.googleblog.com/2022/10/ul2-20b-open-source-unified-language.html)                                                       | 20             | [512, 2048](https://huggingface.co/google/flan-ul2#tldr) | Apache 2.0         |                                                                                                                       |
| Cerebras-GPT | 2023/03 | [Cerebras-GPT](https://huggingface.co/cerebras)                                           | [Cerebras-GPT: Una Familia de Modelos de Lenguaje de Aprendizaje Automático Abiertos, Eficientes en Cálculo y de Gran Tamaño](https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/) ([Artículo](https://arxiv.org/abs/2304.03208)) | 0.111 - 13      | [2048](https://huggingface.co/cerebras/Cerebras-GPT-13B#model-details) | Apache 2.0         | [Cerebras-GPT-1.3B](https://github.com/slai-labs/get-beam/tree/main/examples/cerebras-gpt)                            |
| Open Assistant (Familia Pythia) | 2023/03 | [OA-Pythia-12B-SFT-8](https://huggingface.co/OpenAssistant/pythia-12b-sft-v8-7k-steps), [OA-Pythia-12B-SFT-4](https://huggingface.co/OpenAssistant/oasst-sft-4-pyth

ia-12b-epoch-3.5), [OA-Pythia-12B-SFT-1](https://huggingface.co/OpenAssistant/oasst-sft-1-pythia-12b) | [Democratizando el Alineamiento de Modelos de Lenguaje de Gran Tamaño](https://arxiv.org/abs/2304.07327) | 12     | 2048  | Apache 2.0                | [Pythia-2.8B](https://github.com/slai-labs/get-beam/tree/main/examples/pythia)                                        |
| Pythia       | 2023/04 | [pythia 70M - 12B](https://github.com/EleutherAI/pythia)                                   | [Pythia: Una Suite para Analizar Modelos de Lenguaje de Gran Tamaño en Entrenamiento y Escalado](https://arxiv.org/abs/2304.01373)                                                                    | 0.07 - 12       | [2048](https://arxiv.org/pdf/2304.01373.pdf) | Apache 2.0         |                                                                                                                       |
| Dolly        | 2023/04 | [dolly-v2-12b](https://huggingface.co/databricks/dolly-v2-12b)                            | [Free Dolly: Presentando el Primer Modelo de Lenguaje de Aprendizaje Automático Afinado para Instrucciones Verdaderamente Abierto y Comercialmente Viable](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm)             | 3, 7, 12     | [2048](https://github.com/databrickslabs/dolly#dolly) | MIT                |                                                                                                                       |
| DLite | 2023/05 | [dlite-v2-1_5b](https://huggingface.co/aisquared/dlite-v2-1_5b) | [Anunciando DLite V2: Modelos de Lenguaje de Aprendizaje Automático Abiertos y Livianos que se Pueden Ejecutar en Cualquier Lugar](https://medium.com/ai-squared/announcing-dlite-v2-lightweight-open-llms-that-can-run-anywhere-a852e5978c6e) | 0.124 - 1.5 | 1024 | Apache 2.0         | [DLite-v2-1.5B](https://github.com/slai-labs/get-beam/tree/main/examples/dlite-v2)                                    |
| RWKV         | 2021/08| [RWKV, ChatRWKV](https://github.com/BlinkDL/RWKV-LM#rwkv-parallelizable-rnn-with-transformer-level-llm-performance-pronounced-as-rwakuv-from-4-major-params-r-w-k-v) | [El Modelo de Lenguaje RWKV (y mis Trucos de LLM)](https://github.com/BlinkDL/RWKV-LM)                                           | 0.1 - 14      | [infinito (RNN)](https://github.com/BlinkDL/RWKV-LM#rwkv-parallelizable-rnn-with-transformer-level-llm-performance-pronounced-as-rwakuv-from-4-major-params-r-w-k-v) | Apache 2.0         |                                                                                                                       |
| GPT-J-6B | 2023/06 | [GPT-J

-6B](https://github.com/kingoflolz/mesh-transformer-jax/#gpt-j-6b), [GPT4All-J](https://github.com/nomic-ai/gpt4all#raw-model) | [GPT-J-6B: Transformador JAX de 6B Basado en Aprendizaje Automático](https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/) | 6 | [2048](https://github.com/kingoflolz/mesh-transformer-jax/#gpt-j-6b) | Apache 2.0 |                                                                                                                       |
| GPT-NeoX-20B | 2022/04 | [GPT-NEOX-20B](https://huggingface.co/EleutherAI/gpt-neox-20b) | [GPT-NeoX-20B: Un Modelo de Lenguaje Autoregresivo de Código Abierto](https://arxiv.org/abs/2304.04165) | 20 | [2048](https://huggingface.co/EleutherAI/gpt-neox-20b) | Apache 2.0 |                                                                                                                       |
| Bloom | 2022/11 | [Bloom](https://huggingface.co/bigscience/bloom) | [BLOOM: Un Modelo de Lenguaje Multilingüe de Acceso Abierto con 176 mil millones de Parámetros](https://arxiv.org/abs/2211.05100) | 176 | [2048](https://huggingface.co/bigscience/bloom) |  [OpenRAIL-M v1](https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement) |                                                                                                                       |
| StableLM-Alpha | 2023/04 | [StableLM-Alpha](https://github.com/Stability-AI/StableLM#stablelm-alpha) | [Stability AI Lanza el Primer Modelo de su Suite de Modelos de Lenguaje Estables (StableLM)](https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models) | 3 - 65 | [4096](https://github.com/Stability-AI/StableLM#stablelm-alpha) | CC BY-SA-4.0 |                                                                                                                       |
| FastChat-T5 | 2023/04 | [Estamos emocionados de lanzar FastChat-T5: nuestro chatbot compacto y compatible comercialmente.](https://twitter.com/lmsysorg/status/1652037026705985537?s=20) | [fastchat-t5-3b-v1.0](https://huggingface.co/lmsys/fastchat-t5-3b-v1.0) | 3 | 512 | Apache 2.0 |                                                                                                                       |
| h2oGPT | 2023/05 | [Construyendo el Mejor Modelo de Lenguaje de Aprendizaje Automático de Código Abierto del Mundo: El Recorrido de H2O.ai](https://h2o.ai/blog/building-the-worlds-best-open-source-large-language-model-h2o-ais-journey/) | [h2oGPT](https://github.com/h2oai/h2ogpt) | 12 - 20 | [256 - 2048](https://huggingface.co/h2oai) | Apache 2.0 |                                                                                                                       |
| MPT-7B | 2023/05 | [Introdu
