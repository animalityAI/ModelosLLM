# Open Machine Learning Language Models (Open LLMs)

This repository provides a collection of open-source machine learning language models (LLMs) that are licensed for commercial use (e.g., Apache 2.0, MIT, OpenRAIL-M). Contributions are welcome!

| Language Model | Release Date | Checkpoints | Article/Blog | Parameters (B) | Context Length | License | Try it                                                                                                                |
| --- | --- | --- | --- | --- | --- | --- |---------------------------------------------------------------------------------------------------------------------|
| T5           | 2019/10 |[T5 & Flan-T5](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints), [Flan-T5-xxl (HF)](https://huggingface.co/google/flan-t5-xxl)      | [Exploring the Bounds of Transfer Learning with a Unified Text-to-Text Transformer](https://github.com/google-research/text-to-text-transfer-transformer#released-model-checkpoints) | 0.06 - 11       | 512 | Apache 2.0         | [T5-Large](https://github.com/slai-labs/get-beam/tree/main/examples/t5)                                               |
| UL2          | 2022/10 | [UL2 & Flan-UL2](https://github.com/google-research/google-research/tree/master/ul2#checkpoints), [Flan-UL2 (HF)](https://huggingface.co/google/flan-ul2)          | [UL2 20B: An Open-Source Unified Language Learner](https://ai.googleblog.com/2022/10/ul2-20b-open-source-unified-language.html)                                                       | 20             | 512, 2048 | Apache 2.0         |                                                                                                                       |
| Cerebras-GPT | 2023/03 | [Cerebras-GPT](https://huggingface.co/cerebras)                                           | [Cerebras-GPT: A Family of Open, Compute-Efficient, and Large Language Models](https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/) ([Article](https://arxiv.org/abs/2304.03208)) | 0.111 - 13      | 2048 | Apache 2.0         | [Cerebras-GPT-1.3B](https://github.com/slai-labs/get-beam/tree/main/examples/cerebras-gpt)                            |
| Open Assistant (Pythia Family) | 2023/03 | [OA-Pythia-12B-SFT-8](https://huggingface.co/OpenAssistant/pythia-12b-sft-v8-7k-steps), [OA-Pythia-12B-SFT-4](https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5), [OA-Pythia-12B-SFT-1](https://huggingface.co/OpenAssistant/oasst-sft-1-pythia-12b) | [Democratizing Large Language Model Alignment](https://arxiv.org/abs/2304.07327) | 12 | 2048 | Apache 2.0                | [Pythia-2.8B](https://github.com/slai-labs/get-beam/tree/main/examples/pythia)                                        |
| Pythia       | 2023/04 | [pythia 70M - 12B](https://github.com/EleutherAI/pythia

)                                   | [Pythia: A Suite for Analyzing Large Language Models during Training and Scaling](https://arxiv.org/abs/2304.01373)                                                                    | 0.07 - 12       | 2048 | Apache 2.0         |                                                                                                                       |
| Dolly        | 2023/04 | [dolly-v2-12b](https://huggingface.co/databricks/dolly-v2-12b)                            | [Free Dolly: Introducing the First Open and Commercially Viable Fine-Tuned Language Model for Real Instructions](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm)             | 3, 7, 12     | 2048 | MIT                |                                                                                                                       |
| DLite | 2023/05 | [dlite-v2-1_5b](https://huggingface.co/aisquared/dlite-v2-1_5b) | [Announcing DLite V2: Lightweight Open LLMs that Can Run Anywhere](https://medium.com/ai-squared/announcing-dlite-v2-lightweight-open-llms-that-can-run-anywhere-a852e5978c6e) | 0.124 - 1.5 | 1024 | Apache 2.0         | [DLite-v2-1.5B](https://github.com/slai-labs/get-beam/tree/main/examples/dlite-v2)                                    |
| RWKV         | 2021/08| [RWKV, ChatRWKV](https://github.com/BlinkDL/RWKV-LM#rwkv-parallelizable-rnn-with-transformer-level-llm-performance-pronounced-as-rwakuv-from-4-major-params-r-w-k-v) | [The RWKV Language Model (and My LLM Tricks)](https://github.com/BlinkDL/RWKV-LM)                                           | 0.1 - 14      | Infinite (RNN) | Apache 2.0         |                                                                                                                       |
| GPT-J-6B | 2023/06 | [GPT-J-6B](https://github.com/kingoflolz/mesh-transformer-jax/#gpt-j-6b), [GPT4All-J](https://github.com/nomic-ai/gpt4all#raw-model) | [GPT-J-6B: A 6B JAX-based Transformer Language Model](https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/) | 6 | 2048 | Apache 2.0 |                                                                                                                       |
| GPT-NeoX-20B | 2022/04 | [GPT-NEOX-20B](https://huggingface.co/EleutherAI/gpt-neox-20b) | [GPT-NeoX-20B: An Open-Source Autoregressive Language Model](https://arxiv.org/abs/2304.04165) | 20 | 2048 | Apache 2.0 |                                                                                                                       |
| Bloom | 2022/11 | [Bloom](https://huggingface.co/bigscience/bloom) | [BLOOM: An Open-Access Multilingual Language Model with 176 Billion Parameters](https://arxiv.org/abs/2211.05100) | 176 | 2048 |  [OpenRAIL-M v1](https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement) |                                                                                                                       |
| StableLM-Alpha

 | 2023/04 | [StableLM-Alpha](https://github.com/Stability-AI/StableLM#stablelm-alpha) | [Stability AI Launches the First Model of Its StableLM Language Model Suite](https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models) | 3 - 65 | 4096 | CC BY-SA-4.0 |                                                                                                                       |
| FastChat-T5 | 2023/04 | [FastChat-T5: Our compact and commercially compatible chatbot is now available.](https://twitter.com/lmsysorg/status/1652037026705985537?s=20) | [fastchat-t5-3b-v1.0](https://huggingface.co/lmsys/fastchat-t5-3b-v1.0) | 3 | 512 | Apache 2.0 |                                                                                                                       |
| h2oGPT | 2023/05 | [Building the World's Best Open-Source Large Language Model: H2O.ai's Journey](https://h2o.ai/blog/building-the-worlds-best-open-source-large-language-model-h2o-ais-journey/) | [h2oGPT](https://github.com/h2oai/h2ogpt) | 12 - 20 | 256 - 2048 | Apache 2.0 |                                                                                                                       |
| MPT-7B | 2023/05 | [MPT-7B](https://github.com/?) | [??](??) | ? | ? | ? |                                                                                                                       |
| ANIMALITY.ai LLM | ??? | ??? | ??? | 8.8 Million | 8000 | ??? | ??? |

Please note that the ANIMALITY.ai LLM with 8.8 Million parameters and 8K token size is not listed in the original table.
